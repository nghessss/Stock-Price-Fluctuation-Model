{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Stock Price Fluctation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân tích yêu cầu của công ty\n",
    "## Yêu cầu của công ty\n",
    "Hãy sử dụng dữ liệu được cung cấp trong email này (dữ liệu giá và khối lượng của một vài mã cổ phiếu) để xây dựng một số mô hình dự đoán biến động giá cổ phiếu.\n",
    "**Biến động giá cổ phiếu = Giá cổ phiếu N (phút hoặc giờ hoặc ngày) sau - Giá cổ phiếu hiện tại.**\n",
    "Công ty cung cấp 4 file dữ liệu từ các công ty FPT, MSN, PNJ và VIC bao gồm các trường\n",
    "- Open: Giá mở bán cổ phiếu\n",
    "- High: Giá cao nhất của cổ phiếu trong khoảng thời gian đó.\n",
    "- Low: Giá thấp nhất trong khoảng thời gian đó.\n",
    "- Close: Giá đóng\n",
    "- Volume: Khối lượng giao dịch\n",
    "# Phân tích\n",
    "## Xác định target value\n",
    "$StockPriceFluctuation = Close_{t+1} - Close_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_stock_data(file_paths, N):\n",
    "    dataframes = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure the dataframe has the required columns\n",
    "        required_columns = ['Ticker', 'Date/Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Open Interest']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            print(f\"Warning: {file_path} is missing some required columns. Skipping this file.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert Date/Time to datetime\n",
    "        df['Date/Time'] = pd.to_datetime(df['Date/Time'], format='%m/%d/%Y %H:%M')\n",
    "        \n",
    "        # Sort by Date/Time in descending order (newest first)\n",
    "        df = df.sort_values('Date/Time', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Calculate price fluctuation\n",
    "        df['StockFluctuation'] = df['Close'] - df['Close'].shift(-N)\n",
    "        \n",
    "        # Drop rows with NaN values resulting from the shift operation\n",
    "        df = df.dropna()\n",
    "        \n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# List of file paths (adjust these to match your actual file paths)\n",
    "file_paths = [\n",
    "    'data/FPT.csv',\n",
    "    'data/MSN.csv',\n",
    "    'data/PNJ.csv',\n",
    "    'data/VIC.csv'\n",
    "]\n",
    "\n",
    "# Set N for the number of periods to look ahead (e.g., 1 for next minute, 5 for 5 minutes ahead, etc.)\n",
    "N = 1\n",
    "\n",
    "# Process the data\n",
    "result_df = load_and_process_stock_data(file_paths, N)\n",
    "\n",
    "# Display the first few rows and info of the resulting dataframe\n",
    "print(result_df.head())\n",
    "print(result_df.info())\n",
    "\n",
    "# Optional: Save the processed data to a new CSV file\n",
    "result_df.to_csv('processed_stock_data.csv', index=False)\n",
    "print(\"Processed data saved to 'processed_stock_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Group by Ticker and display summary statistics\n",
    "summary_stats = result_df.groupby('Ticker').agg({\n",
    "    'Open': 'mean',\n",
    "    'High': 'max',\n",
    "    'Low': 'min',\n",
    "    'Close': 'mean',\n",
    "    'Volume': 'sum',\n",
    "    'StockFluctuation': ['mean', 'std']\n",
    "})\n",
    "print(\"\\nSummary Statistics by Ticker:\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Define feature columns to scale\n",
    "feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "# Apply scaling for each company\n",
    "scaled_dfs = []\n",
    "for ticker, group in result_df.groupby('Ticker'):\n",
    "    # Scale features\n",
    "    group[feature_columns] = feature_scaler.fit_transform(group[feature_columns])\n",
    "    # Scale target (StockFluctuation)\n",
    "    group['StockFluctuation'] = target_scaler.fit_transform(group[['StockFluctuation']])\n",
    "    scaled_dfs.append(group)\n",
    "\n",
    "# Combine back the scaled data\n",
    "scaled_combined_df = pd.concat(scaled_dfs, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time Series Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, feature_columns, target_column, lookback):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[feature_columns].iloc[i:i+lookback].values)\n",
    "        y.append(data[target_column].iloc[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_list, y_list = [], []\n",
    "lookback_period = 30\n",
    "\n",
    "for ticker, group in scaled_combined_df.groupby('Ticker'):\n",
    "    X, y = create_sequences(group, feature_columns, 'StockFluctuation', lookback_period)\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "\n",
    "# Concatenate all the sequences from different companies\n",
    "X_combined = np.concatenate(X_list, axis=0)\n",
    "y_combined = np.concatenate(y_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layers\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units=50))\n",
    "\n",
    "# Output layer to predict stock fluctuation\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_rescaled = target_scaler.inverse_transform(y_pred)\n",
    "y_test_rescaled = target_scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "mae_percentage = (mae / np.mean(y_test)) * 100\n",
    "print(\"Mean absolute error on test set: {:.2f}%\".format(mae_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test_rescaled, y_pred_rescaled)\n",
    "print(\"R2 score:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
